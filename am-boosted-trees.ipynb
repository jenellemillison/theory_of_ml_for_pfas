{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError:\n",
    "    %pip install -q xgboost\n",
    "try:\n",
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    from sklearn.mixture import GaussianMixture  # Using GMM for clustering\n",
    "    from sklearn.manifold import LocallyLinearEmbedding  # For dimensionality reduction\n",
    "except ImportError:\n",
    "    %pip install sklearn\n",
    "try:\n",
    "    from collections import Counter\n",
    "except ImportError:\n",
    "    %pip install collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFAs Data:\n",
      "      NAWQA_ID       DATE  TIME 4_2 FTS-RMK  4_2 FTS-VA 6_2 FTS-RMK  \\\n",
      "0  MIAMSUS1-01  8/21/2019  1200           <         8.0           <   \n",
      "1  MIAMSUS1-02   7/1/2019  1100           <         7.7           <   \n",
      "2  MIAMSUS1-03   7/2/2019  1100           <         7.7           <   \n",
      "3  MIAMSUS1-04   7/9/2019  1100           <         8.0           <   \n",
      "4  MIAMSUS1-05  7/10/2019  1100           <         8.0           <   \n",
      "\n",
      "   6_2 FTS-VA 8_2 FTS-RMK  8_2 FTS-VA N-EtFOSAA-RMK  ...  PFPeS-RMK PFPeS-VA  \\\n",
      "0         8.0           <         8.0             <  ...          <      4.0   \n",
      "1         7.7           <         7.7             <  ...          <      3.8   \n",
      "2         7.7           <         7.7             <  ...          <      3.8   \n",
      "3         8.0           <         8.0             <  ...        NaN      7.8   \n",
      "4         8.0           <         8.0             <  ...          <      4.0   \n",
      "\n",
      "   PFPeA-RMK PFPeA-VA  PFTeDA-RMK PFTeDA-VA  PFTrDA-RMK PFTrDA-VA  PFUnA-RMK  \\\n",
      "0          n      1.7           <       4.0           <       4.0          <   \n",
      "1          <      3.8           <       3.8           <       3.8          <   \n",
      "2          <      3.8           <       3.8           <       3.8          <   \n",
      "3          <      4.0           <       4.0           <       4.0          <   \n",
      "4          <      4.0           <       4.0           <       4.0          <   \n",
      "\n",
      "  PFUnA-VA  \n",
      "0      4.0  \n",
      "1      3.8  \n",
      "2      3.8  \n",
      "3      4.0  \n",
      "4      4.0  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "# first, re-create clusters. RF was run on top features predicting clusters\n",
    "# from Courtney\n",
    "\n",
    "# data import: ENV\n",
    "# assumes this script saved one folder up of /data\n",
    "pfa_data = pd.read_csv('./data/raw_data/PFAS_ENV.csv')\n",
    "\n",
    "# Display the first few rows of each dataset for exploration\n",
    "print(\"PFAs Data:\")\n",
    "print(pfa_data.head())\n",
    "\n",
    "# Convert 'DATE' column to datetime format\n",
    "pfa_data['DATE'] = pd.to_datetime(pfa_data['DATE'], format='%m/%d/%Y')\n",
    "\n",
    "#CLUSTERING MODEL (GMM with 8 Clusters from AIC/BIC)\n",
    "\n",
    "# Select numeric columns for clustering\n",
    "numeric_columns = pfa_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Impute missing values with median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "pfa_data_imputed = imputer.fit_transform(pfa_data[numeric_columns])\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "pfa_scaled = scaler.fit_transform(pfa_data_imputed)\n",
    "\n",
    "# Apply Gaussian Mixture Model with the optimal number of clusters (8 clusters)\n",
    "gmm_final = GaussianMixture(n_components=8, random_state=42)\n",
    "pfa_data['Cluster'] = gmm_final.fit_predict(pfa_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n"
     ]
    }
   ],
   "source": [
    "print(np.sort(pfa_data.Cluster.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_pfas = ['PFOS-VA', 'PFBA-VA', 'PFHxS-VA', 'PFOA-VA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution: Counter({1: 96, 2: 81, 6: 36, 4: 34, 0: 4, 7: 1, 5: 1, 3: 1})\n"
     ]
    }
   ],
   "source": [
    "# X, y declarations\n",
    "\n",
    "X = pfa_data[key_pfas]  # Using the top features as the predictors\n",
    "y = pfa_data['Cluster']  # Cluster labels as the target\n",
    "\n",
    "# Count instances of each class\n",
    "class_counts = Counter(y)\n",
    "print(\"Class distribution:\", class_counts)\n",
    "\n",
    "# DROP clusters 3, 5, 7 due to sparsity AND 0\n",
    "valid_classes = [cls for cls, count in class_counts.items() if count >= 5]\n",
    "\n",
    "# Filter out rows with invalid classes\n",
    "mask = np.isin(y, valid_classes)\n",
    "X, y = X[mask], y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 96, 2: 81, 4: 34, 6: 36})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratify y for equal class proportions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 67, 2: 56, 6: 25, 4: 24})\n",
      "Counter({1: 29, 2: 25, 6: 11, 4: 10})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(y_train))\n",
    "print(Counter(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 2, 1, 1, 0, 0, 0, 3, 1, 2, 0,\n",
       "       0, 1, 0, 1, 1, 0, 2, 3, 1, 1, 1, 2, 0, 0, 0, 2, 0, 1, 0, 2, 0, 1,\n",
       "       0, 2, 1, 1, 2, 1, 0, 3, 1, 0, 3, 1, 0, 1, 0, 3, 3, 2, 1, 3, 0, 0,\n",
       "       1, 2, 1, 2, 0, 1, 0, 0, 1, 1, 0, 0, 3, 0, 1, 0, 3, 0, 0, 1, 0, 2,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 2, 0, 1, 0,\n",
       "       0, 2, 0, 1, 1, 0, 1, 2, 3, 0, 0, 3, 0, 2, 0, 2, 0, 0, 0, 1, 3, 3,\n",
       "       1, 3, 1, 3, 3, 1, 3, 3, 1, 1, 1, 0, 2, 3, 2, 3, 0, 0, 2, 0, 3, 0,\n",
       "       1, 0, 1, 1, 2, 0, 1, 0, 3, 3, 0, 2, 1, 1, 0, 2, 3, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-index the class labels to ensure they start from 0\n",
    "# Create a mapping of old labels to new ones starting from 0\n",
    "class_mapping = {label: idx for idx, label in enumerate(np.unique(y_train))}\n",
    "y_train = np.array([class_mapping[label] for label in y_train])\n",
    "y_test = np.array([class_mapping[label] for label in y_test])\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Predict cluster based off key_pfas levels\n",
    "This was already done as a baseline using Random Forest. This experiment aims to improve upon this technique by using boosted regression trees. The logic is that the sequential learning nature of this approach could improve accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86        29\n",
      "           1       0.79      0.88      0.83        25\n",
      "           2       0.50      0.50      0.50        10\n",
      "           3       0.88      0.64      0.74        11\n",
      "\n",
      "    accuracy                           0.79        75\n",
      "   macro avg       0.76      0.72      0.73        75\n",
      "weighted avg       0.79      0.79      0.78        75\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# init XBGClassifier\n",
    "# optimization: softmax\n",
    "# eval: min log loss\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective=\"multi:softmax\", eval_metric=\"mlogloss\", use_label_encoder=False)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'gamma': 5, 'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 200, 'subsample': 0.8}\n",
      "Best Accuracy: 0.8662028634805404\n"
     ]
    }
   ],
   "source": [
    "# GRID SEARCH for hyperparameter tuning\n",
    "xgb_clf = xgb.XGBClassifier(objective=\"multi:softmax\", eval_metric=\"mlogloss\", use_label_encoder=False)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5], # 7\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,  # 3-fold cross-validation, limited due to memory constraints\n",
    "    verbose=2,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the optimal boosted tree setup includes:\n",
    "- depth = 3 (max depth of a tree)\n",
    "- learning_rate = 0.01 (step size, prevent overfitting)\n",
    "- n_estimators = 200 \n",
    "- gamma = 5 (min loss reduction for a split)\n",
    "- subsample = 0.8 (fraction of samples for growing trees)\n",
    "- colsample_bytree = 0.8 (fraction of features per tree)\n",
    "\n",
    "Achieving 86.62% accuracy\n",
    "\n",
    "### TODO: get more metrics or plots showcasing result of experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Predict amounts of key_pfas based off env/pharma/geo data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment aims to predict concentrations of key PFAS by regressing on the presence or amounts of external features, such as pharmaceutical compunds, geographical features, and other environmental features.\n",
    "\n",
    "\n",
    "TODO:\n",
    "- figure out best way to choose features for selection. Maybe running a broad linear regression and evaluating feature importance?\n",
    "- once features whittled down, join data and extrac X (features) and y (target concentration)\n",
    "- run train/test split on joined data\n",
    "- run xgboost baseline + grid search\n",
    "- plot/summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join cols: NAWQA_ID , maybe also DATE and TIME ?\n",
    "\n",
    "# what we want: get key PFAS from ENV --> join to something like Geospatial, inorganics, pharma and choose some predictors --> \n",
    "# --> predict amounts of key PFAS using joined features as predictors. Method of prediction: regression trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en685648)",
   "language": "python",
   "name": "en685648"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
